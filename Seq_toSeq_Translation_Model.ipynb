{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qT0vGMICWk6_"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import re\n",
        "import tensorflow as tf\n",
        "import unicodedata\n",
        "\n",
        "from google.colab import files\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset\n",
        "!wget https://raw.githubusercontent.com/futuremojo/nlp-demystified/main/datasets/hun_eng_pairs/hun_eng_pairs_train.txt\n",
        "!wget https://raw.githubusercontent.com/futuremojo/nlp-demystified/main/datasets/hun_eng_pairs/hun_eng_pairs_val.txt\n",
        "!wget https://raw.githubusercontent.com/futuremojo/nlp-demystified/main/datasets/hun_eng_pairs/hun_eng_pairs_test.txt"
      ],
      "metadata": {
        "id": "cCTIBSjwWvDy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac442f4d-c495-4b63-fde1-1879a6706658"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-27 16:41:44--  https://raw.githubusercontent.com/futuremojo/nlp-demystified/main/datasets/hun_eng_pairs/hun_eng_pairs_train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5518306 (5.3M) [text/plain]\n",
            "Saving to: ‘hun_eng_pairs_train.txt.1’\n",
            "\n",
            "hun_eng_pairs_train 100%[===================>]   5.26M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-09-27 16:41:44 (84.3 MB/s) - ‘hun_eng_pairs_train.txt.1’ saved [5518306/5518306]\n",
            "\n",
            "--2024-09-27 16:41:44--  https://raw.githubusercontent.com/futuremojo/nlp-demystified/main/datasets/hun_eng_pairs/hun_eng_pairs_val.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 646226 (631K) [text/plain]\n",
            "Saving to: ‘hun_eng_pairs_val.txt.1’\n",
            "\n",
            "hun_eng_pairs_val.t 100%[===================>] 631.08K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-09-27 16:41:44 (21.5 MB/s) - ‘hun_eng_pairs_val.txt.1’ saved [646226/646226]\n",
            "\n",
            "--2024-09-27 16:41:44--  https://raw.githubusercontent.com/futuremojo/nlp-demystified/main/datasets/hun_eng_pairs/hun_eng_pairs_test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 646226 (631K) [text/plain]\n",
            "Saving to: ‘hun_eng_pairs_test.txt.1’\n",
            "\n",
            "hun_eng_pairs_test. 100%[===================>] 631.08K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-09-27 16:41:44 (14.7 MB/s) - ‘hun_eng_pairs_test.txt.1’ saved [646226/646226]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('hun_eng_pairs_train.txt') as file:\n",
        "  train = [line.rstrip() for line in file]\n",
        "\n",
        "with open('hun_eng_pairs_val.txt') as file:\n",
        "  val = [line.rstrip() for line in file]\n",
        "\n",
        "with open('hun_eng_pairs_test.txt') as file:\n",
        "  test = [line.rstrip() for line in file]"
      ],
      "metadata": {
        "id": "dnPD07oHbqor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train[0])\n",
        "print(val[0])\n",
        "print(test[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYgZUaqcb0U7",
        "outputId": "e29bd2a7-a475-4972-bbd4-c6f6a26a2eb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teszek rá, mit mondasz!<sep>I don't care what you say.\n",
            "Abbahagyhatom, ha zavar.<sep>If it bothers you, I can stop doing this.\n",
            "Abbahagyhatom, ha zavar.<sep>If it bothers you, I can stop doing this.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train = [], []\n",
        "\n",
        "SEPERATOR = \"<sep>\"\n",
        "\n",
        "for sample in train:\n",
        "  x = sample.split(SEPERATOR)\n",
        "  x_train.append(x[0])\n",
        "  y_train.append(x[1])"
      ],
      "metadata": {
        "id": "08fuP8crcJ5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train[0])\n",
        "print(y_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6Z3PsLV873T",
        "outputId": "d4cca00b-a64d-4bb9-c00d-06f9425d1a5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teszek rá, mit mondasz!\n",
            "I don't care what you say.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unicode normalization\n",
        "def normalize_unicode(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def preprocess_sentence(s):\n",
        "  s = normalize_unicode(s)\n",
        "  s = re.sub(r\"([?.!,¿])\", r\" \\1 \", s)\n",
        "  s = re.sub(r'[\" \"]+', \" \", s)\n",
        "  s = s.strip()\n",
        "  return s"
      ],
      "metadata": {
        "id": "5eKFSE4p9FWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_preprocessed = [preprocess_sentence(s) for s in x_train]\n",
        "y_train_preprocessed = [preprocess_sentence(s) for s in y_train]"
      ],
      "metadata": {
        "id": "2VQAjfKCBYtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tag_target_sentences(sentences):\n",
        "  tagged_sentences = map(lambda s: (' ').join(['<sos>', s, '<eos>']), sentences)\n",
        "  return list(tagged_sentences)"
      ],
      "metadata": {
        "id": "iLBcfBaiMFSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_preprocessed_tagged = tag_target_sentences(y_train_preprocessed)"
      ],
      "metadata": {
        "id": "B-P-27_KMarK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create tokenizers\n",
        "source_lang_tokeniser = tf.keras.preprocessing.text.Tokenizer(oov_token=\"<oov>\", filters='\"#$%&()*+-/:;=@[\\\\]^_`{|}~\\t\\n')\n",
        "target_lang_tokeniser = tf.keras.preprocessing.text.Tokenizer(oov_token=\"<oov>\", filters='\"#$%&()*+-/:;=@[\\\\]^_`{|}~\\t\\n')\n",
        "\n",
        "source_lang_tokeniser.fit_on_texts(x_train_preprocessed)\n",
        "target_lang_tokeniser.fit_on_texts(y_train_preprocessed_tagged)"
      ],
      "metadata": {
        "id": "E0LBw8jtBk26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encoder_input_sequences = source_lang_tokeniser.texts_to_sequences(x_train_preprocessed)"
      ],
      "metadata": {
        "id": "b6SQfmW-DRi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_decoder_inputs_targets(sentences, tokenizer):\n",
        "\n",
        "  sentence_sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "  decoder_input_sequences, decoder_target_sequences = [], []\n",
        "  for sentence in sentence_sequences:\n",
        "\n",
        "    decoder_input_sequences.append(sentence[:-1])\n",
        "    decoder_target_sequences.append(sentence[1:])\n",
        "\n",
        "  return decoder_input_sequences, decoder_target_sequences"
      ],
      "metadata": {
        "id": "tb25HGQ0C877"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_decoder_input, test_decoder_target = generate_decoder_inputs_targets(y_train_preprocessed_tagged, target_lang_tokeniser)"
      ],
      "metadata": {
        "id": "4bLE9aArkVeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_decoder_input_sequences, train_decoder_target_sequences = generate_decoder_inputs_targets(y_train_preprocessed_tagged, target_lang_tokeniser)"
      ],
      "metadata": {
        "id": "_xIidfzWD4xT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_ENC_LEN = max([len(sentence) for sentence in train_encoder_input_sequences])\n",
        "MAX_DEC_LEN = max([len(sentence) for sentence in train_decoder_target_sequences])\n",
        "\n",
        "print(MAX_ENC_LEN)\n",
        "print(MAX_DEC_LEN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZEg74HyPOjF",
        "outputId": "4e1feefb-253c-4602-944f-577185fa0658"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "37\n",
            "34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_encoder_input_sequences_padded = pad_sequences(train_encoder_input_sequences, maxlen=MAX_ENC_LEN, padding='post')\n",
        "train_decoder_input_sequences_padded = pad_sequences(train_decoder_input_sequences, maxlen=MAX_DEC_LEN, padding='post')\n",
        "train_decoder_target_sequences_padded = pad_sequences(train_decoder_target_sequences, maxlen=MAX_DEC_LEN, padding='post')"
      ],
      "metadata": {
        "id": "7gExPJR5QQvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_dataset(dataset, SEPERATOR=\"<sep>\"):\n",
        "\n",
        "  x, y = [], []\n",
        "  for sample in dataset:\n",
        "    data = sample.split(SEPERATOR)\n",
        "    x.append(data[0])\n",
        "    y.append(data[1])\n",
        "\n",
        "  x_preprocessed = [preprocess_sentence(s) for s in x]\n",
        "  y_preprocessed = [preprocess_sentence(s) for s in y]\n",
        "\n",
        "  y_preprocessed_tagged = tag_target_sentences(y_preprocessed)\n",
        "\n",
        "  encoder_sequences = source_lang_tokeniser.texts_to_sequences(x_preprocessed)\n",
        "  decoder_input_sequences, decoder_target_sequences = generate_decoder_inputs_targets(y_preprocessed_tagged, target_lang_tokeniser)\n",
        "\n",
        "  encoder_input_sequences_padded = pad_sequences(encoder_sequences, maxlen=MAX_ENC_LEN, padding='post')\n",
        "  decoder_input_sequences_padded = pad_sequences(decoder_input_sequences, maxlen=MAX_DEC_LEN, padding='post')\n",
        "  decoder_target_sequences_padded = pad_sequences(decoder_target_sequences, maxlen=MAX_DEC_LEN, padding='post')\n",
        "\n",
        "  return encoder_input_sequences_padded, decoder_input_sequences_padded, decoder_target_sequences_padded"
      ],
      "metadata": {
        "id": "Fc7H1HEFQXr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_encoder_input_sequences_padded, val_decoder_input_sequences_padded, val_decoder_target_sequences_padded = preprocess_dataset(val)"
      ],
      "metadata": {
        "id": "7XetV28WTPMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_vocab_size = len(source_lang_tokeniser.word_index) + 1 # +1 is for the <oov> token\n",
        "target_vocab_size = len(target_lang_tokeniser.word_index) + 1\n",
        "\n",
        "embedding_dim = 128\n",
        "hidden_units = 256\n",
        "batch_size = 32\n",
        "dropout_rate = 0.2"
      ],
      "metadata": {
        "id": "VR_F7PbfTbCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_units):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, name=\"encoder_embedding_layer\")\n",
        "    self.lstm = tf.keras.layers.LSTM(units=hidden_units, return_sequences=True, return_state=True, name=\"encoder_lstm_layer\")\n",
        "\n",
        "  def call(self, input):\n",
        "    input_embeddings = self.embedding(input)\n",
        "    output_sequences, hidden_state, cell_state = self.lstm(input_embeddings)\n",
        "    return output_sequences, hidden_state, cell_state"
      ],
      "metadata": {
        "id": "621oTiTv34PP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_encoder = Encoder(source_vocab_size, embedding_dim, hidden_units)"
      ],
      "metadata": {
        "id": "o3MdZLCh_bq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_input = train_encoder_input_sequences_padded[:3]\n",
        "sample_output_sequences, sample_hidden_state, sample_cell_state = source_encoder(sample_input)\n",
        "\n",
        "print(sample_output_sequences.shape)\n",
        "print(sample_hidden_state.shape)\n",
        "print(sample_cell_state.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOCOJikR_1tP",
        "outputId": "243cad04-6763-4ebb-8019-360ac9473297"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 37, 256)\n",
            "(3, 256)\n",
            "(3, 256)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LuongAttention(tf.keras.Model):\n",
        "  def __init__(self, hidden_units):\n",
        "    super(LuongAttention, self).__init__()\n",
        "\n",
        "    self.W_scoring_function = tf.keras.layers.Dense(units=hidden_units, name=\"W_scoring_function\")\n",
        "\n",
        "  def call(self, inputs):\n",
        "    encoder_output_sequences, decoder_output = inputs\n",
        "    z = self.W_scoring_function(encoder_output_sequences)\n",
        "    attn_scores = tf.matmul(decoder_output, z, transpose_b=True)\n",
        "    attn_weights = tf.keras.activations.softmax(attn_scores, axis= -1)   # note the -1\n",
        "    context_vector = tf.matmul(attn_weights, encoder_output_sequences)\n",
        "    return context_vector, attn_weights"
      ],
      "metadata": {
        "id": "j5nAt5IKAmBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_units):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.attention = LuongAttention(hidden_units)\n",
        "    self.w = tf.keras.layers.Dense(units=hidden_units, activation=\"tanh\", name=\"attention_tanh_output_layer\")\n",
        "    self.dense = tf.keras.layers.Dense(units=vocab_size, name=\"decoder_output_layer\") # map the LSTM output to target lang vocab size\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, name=\"decoder_embedding_layer\")\n",
        "    self.lstm = tf.keras.layers.LSTM(units=hidden_units, return_sequences=True, return_state=True, name=\"decoder_lstm_layer\")\n",
        "\n",
        "\n",
        "  def call(self, inputs):\n",
        "\n",
        "    decoder_input, encoder_output_sequences, hidden_state, cell_state = inputs\n",
        "    # print(\"decoder_input Shape: \", decoder_input.shape)\n",
        "    input_embeddings = self.embedding(decoder_input)\n",
        "    # print(\"Input Embeddings Shape: \", input_embeddings.shape)\n",
        "    decoder_output, decoder_hidden_state, decoder_cell_state = self.lstm(input_embeddings, initial_state=[hidden_state, cell_state])\n",
        "    # print(\"Decoder Output Shape: \", decoder_output.shape)\n",
        "\n",
        "    context_vector, attention_weights = self.attention([encoder_output_sequences, decoder_output])\n",
        "    # print(\"Context Vector Shape: \", context_vector.shape)\n",
        "    # print(\"Attention Weights Shape: \", attention_weights.shape)\n",
        "\n",
        "    z = self.w(tf.concat([tf.squeeze(context_vector,1), tf.squeeze(decoder_output,1)], axis=1))\n",
        "    # print(\"Z Shape: \", z.shape)\n",
        "    logits = self.dense(z)\n",
        "    # print(\"Logits Shape: \", logits.shape)\n",
        "\n",
        "    return logits, decoder_hidden_state, decoder_cell_state, attention_weights"
      ],
      "metadata": {
        "id": "BUbxJ5WIG9ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_decoder = Decoder(target_vocab_size, embedding_dim, hidden_units)"
      ],
      "metadata": {
        "id": "31jIlg1QL6Ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_decoder_input = train_decoder_input_sequences_padded[:3, 1]\n",
        "sample_decoder_input = tf.expand_dims(sample_decoder_input, 1)\n",
        "logits, decoder_hidden_state, decoder_cell_state, attention_weights = target_decoder([sample_decoder_input, sample_output_sequences, sample_hidden_state, sample_cell_state])"
      ],
      "metadata": {
        "id": "m_j4dD65ME80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageTranslator(tf.keras.Model):\n",
        "  def __init__(self, source_encoder, target_decoder):\n",
        "    super(LanguageTranslator, self).__init__()\n",
        "\n",
        "    self.source_encoder = source_encoder\n",
        "    self.target_decoder = target_decoder\n",
        "\n",
        "  # This method will be called by model.fit for each batch.\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "    encoder_input_sequences, decoder_input_sequences, decoder_target_sequences = inputs\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      encoder_output_sequences, encoder_hidden_state, encoder_cell_state = self.source_encoder(encoder_input_sequences)\n",
        "\n",
        "      current_decoder_hidden_state = encoder_hidden_state\n",
        "      current_decoder_cell_state = encoder_cell_state\n",
        "\n",
        "      total_loss = 0\n",
        "\n",
        "      # Iterating over each sequence of a sentence (datapoint)\n",
        "      for t in range(decoder_target_sequences.shape[1]):\n",
        "\n",
        "        # Input to decoder at each time step is a single token of target language\n",
        "        decoder_input = decoder_input_sequences[:, t]\n",
        "        decoder_input = tf.expand_dims(decoder_input, 1)\n",
        "        logits, decoder_hidden_state, decoder_cell_state, _ = self.target_decoder([decoder_input, encoder_output_sequences, current_decoder_hidden_state, current_decoder_cell_state])\n",
        "\n",
        "        # The loss is now accumulated through the whole batch\n",
        "        loss = self.loss(decoder_target_sequences[:, t], logits)\n",
        "        total_loss += loss\n",
        "\n",
        "        current_decoder_hidden_state = decoder_hidden_state\n",
        "        current_decoder_cell_state = decoder_cell_state\n",
        "\n",
        "\n",
        "      variables = self.source_encoder.trainable_variables + self.target_decoder.trainable_variables\n",
        "      gradients = tape.gradient(total_loss, variables)\n",
        "      self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "      return {\"loss\": total_loss/decoder_target_sequences.shape[1]}\n"
      ],
      "metadata": {
        "id": "5mFlERByU08P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kuVXEpEmYlJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_func(targets, logits):\n",
        "  ce_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "  mask = tf.cast(tf.math.not_equal(targets, 0), tf.float32)\n",
        "\n",
        "  return ce_loss(targets, logits, sample_weight=mask)"
      ],
      "metadata": {
        "id": "uT5rySfgaIYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_encoder_input_sequences_padded, train_decoder_input_sequences_padded, train_decoder_target_sequences_padded))\n",
        "train_dataset = train_dataset.batch(batch_size, drop_remainder=True)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_encoder_input_sequences_padded, val_decoder_input_sequences_padded, val_decoder_target_sequences_padded))\n",
        "val_dataset = val_dataset.batch(batch_size, drop_remainder=True)"
      ],
      "metadata": {
        "id": "TMbWHL7PaLbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(source_vocab_size, embedding_dim, hidden_units)\n",
        "decoder = Decoder(target_vocab_size, embedding_dim, hidden_units)\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "translator = LanguageTranslator(encoder, decoder)\n",
        "translator.compile(optimizer=optimizer, loss=loss_func)"
      ],
      "metadata": {
        "id": "xmba_zpIaWJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "\n",
        "history = translator.fit(train_dataset, epochs=epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1ZbGJk9akIG",
        "outputId": "4c6a26fa-1076-41f5-de96-69b3a8169afa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m2770/2770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 66ms/step - loss: 0.8876\n",
            "Epoch 2/10\n",
            "\u001b[1m2770/2770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 64ms/step - loss: 0.6152\n",
            "Epoch 3/10\n",
            "\u001b[1m2770/2770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 64ms/step - loss: 0.4601\n",
            "Epoch 4/10\n",
            "\u001b[1m2770/2770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 64ms/step - loss: 0.3447\n",
            "Epoch 5/10\n",
            "\u001b[1m2770/2770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 64ms/step - loss: 0.2615\n",
            "Epoch 6/10\n",
            "\u001b[1m2770/2770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 64ms/step - loss: 0.2016\n",
            "Epoch 7/10\n",
            "\u001b[1m2770/2770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 64ms/step - loss: 0.1583\n",
            "Epoch 8/10\n",
            "\u001b[1m2770/2770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 64ms/step - loss: 0.1270\n",
            "Epoch 9/10\n",
            "\u001b[1m2770/2770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 64ms/step - loss: 0.1042\n",
            "Epoch 10/10\n",
            "\u001b[1m2770/2770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 64ms/step - loss: 0.0865\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save the encoder and decoder weights\n",
        "encoder.save_weights('encoder.weights.h5')\n",
        "decoder.save_weights('decoder.weights.h5')\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "files.download('encoder.weights.h5')\n",
        "files.download('decoder.weights.h5')"
      ],
      "metadata": {
        "id": "RmlEBob5cPv7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f45793ec-8761-49d6-8f26-5b5cb006bfce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_14d8326c-e055-46b9-8d9c-c5a9285ae77d\", \"encoder.weights.h5\", 21319920)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a14cc045-f038-4b88-abd7-b050829b51a9\", \"decoder.weights.h5\", 18642272)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(encoder, decoder, sentence, maxLen=30):\n",
        "\n",
        "  x_preprocessed = preprocess_sentence(sentence)\n",
        "  encoder_sequences = source_lang_tokeniser.texts_to_sequences(x_preprocessed)\n",
        "\n",
        "  encoder_input_sequences_padded = pad_sequences(encoder_sequences, maxlen=MAX_ENC_LEN, padding='post')\n",
        "\n",
        "  encoder_output_sequences, encoder_hidden_state, encoder_cell_state = encoder(encoder_input_sequences_padded)\n",
        "\n",
        "  current_decoder_hidden_state = encoder_hidden_state\n",
        "  current_decoder_cell_state = encoder_cell_state\n",
        "\n",
        "  currLen = 0\n",
        "  current_word = '<sos>'\n",
        "  translated_sentence = []\n",
        "\n",
        "  while currLen < maxLen:\n",
        "\n",
        "    # print(\"input word: \", current_word)\n",
        "\n",
        "    decoder_input = np.zeros((1,1))\n",
        "    decoder_input[0, 0] = target_lang_tokeniser.word_index[current_word]\n",
        "    logits, decoder_hidden_state, decoder_cell_state, _ = decoder([decoder_input, encoder_output_sequences, current_decoder_hidden_state, current_decoder_cell_state])\n",
        "    predicted_id = tf.argmax(logits, axis= -1)\n",
        "\n",
        "    current_word = target_lang_tokeniser.index_word[predicted_id.numpy()[0]]\n",
        "\n",
        "    # print(\"predicted word: \", current_word)\n",
        "    # print(\"\\n\")\n",
        "\n",
        "    if current_word == '<eos>':\n",
        "      return translated_sentence\n",
        "\n",
        "    translated_sentence.append(current_word)\n",
        "\n",
        "    decoder_input = predicted_id\n",
        "    current_decoder_hidden_state = decoder_hidden_state\n",
        "    current_decoder_cell_state = decoder_cell_state\n",
        "\n",
        "    currLen += 1\n",
        "\n",
        "  return translated_sentence"
      ],
      "metadata": {
        "id": "y49HPtm5YWY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hungarian_sentence1 = \"Teszek rá, mit mondasz!\"\n",
        "hungarian_sentence2 = \"Abbahagyhatom, ha zavar.\"\n",
        "english_sentence1 = translate(encoder, decoder, hungarian_sentence1)\n",
        "english_sentence2 = translate(encoder, decoder, hungarian_sentence2)\n",
        "print(english_sentence1)\n",
        "print(english_sentence2)"
      ],
      "metadata": {
        "id": "4HRADc9_fkor"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}